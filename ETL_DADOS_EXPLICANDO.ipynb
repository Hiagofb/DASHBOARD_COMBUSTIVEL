{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3c75abee",
   "metadata": {},
   "source": [
    "---\n",
    "#  Mini ETL com Python para alimentar dashboard\n",
    "\n",
    "Este notebook explica o processo de ETL (Extra√ß√£o, Transforma√ß√£o e Carga) dos dados de pre√ßos dos combust√≠veis no Brasil, utilizados em um dashboard desenvolvido no Power BI. O objetivo √© consolidar diversos arquivos CSV semestrais em uma √∫nica base limpa e padronizada para an√°lise visual, permitindo que novos dados sejam facilmente inseridos no fluxo sem necessidade de altera√ß√µes manuais."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f56ad698",
   "metadata": {},
   "source": [
    "---\n",
    "### Importa√ß√£o de bibliotecas\n",
    "\n",
    "Utilizamos as seguintes bibliotecas:\n",
    "\n",
    "- `os`: para manipula√ß√£o e navega√ß√£o entre arquivos e pastas do sistema.\n",
    "- `pandas`: para leitura, transforma√ß√£o e consolida√ß√£o dos dados tabulares (CSV).\n",
    "- `chardet`: para detectar automaticamente o encoding de cada arquivo CSV, garantindo a leitura correta das informa√ß√µes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7a8b7aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import chardet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78d8e9cb",
   "metadata": {},
   "source": [
    "---\n",
    "### Listagem dos Arquivos CSV\n",
    "Nesta etapa, criamos uma lista com os nomes de todos os arquivos localizados na pasta `DADOS`. Cada arquivo representa um per√≠odo semestral de coleta de pre√ßos de combust√≠veis disponibilizado pela ANP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fc0fae2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arquivos encontrados: 36\n"
     ]
    }
   ],
   "source": [
    "caminho = r\"C:\\\\Users\\\\hiago\\\\OneDrive\\\\Documentos\\\\CURSOS\\\\PORTIFOLIO\\\\DASHBOARD_PRECO_COMBUSTIVEL\"\n",
    "lista_arquivos = os.listdir(f\"{caminho}/DADOS\")\n",
    "print(f\"Arquivos encontrados: {len(lista_arquivos)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a8300ad",
   "metadata": {},
   "source": [
    "---\n",
    "### Consolida√ß√£o dos Arquivos CSV\n",
    "\n",
    "Nesta etapa, percorremos todos os arquivos da pasta `DADOS` e executamos as seguintes a√ß√µes para consolidar os dados em um √∫nico DataFrame final:\n",
    "\n",
    "- **Detectar o encoding de cada arquivo CSV**  \n",
    "  Utilizamos a biblioteca `chardet` para identificar automaticamente a codifica√ß√£o do texto, evitando problemas com acentos e caracteres especiais.\n",
    "\n",
    "- **Ler os arquivos CSV com o encoding correto**  \n",
    "  Cada arquivo √© lido individualmente com `pandas.read_csv`, utilizando o separador `;`.\n",
    "\n",
    "- **Remover colunas desnecess√°rias**  \n",
    "  Ap√≥s a leitura, mantemos apenas as colunas definidas na lista `colunas_necess`, eliminando qualquer informa√ß√£o extra que n√£o ser√° utilizada na an√°lise.\n",
    "\n",
    "- **Concatenar os dados em um √∫nico DataFrame**  \n",
    "  Cada arquivo processado √© anexado ao `df_final`, formando uma base √∫nica consolidada para ser utilizada no Power BI.\n",
    "\n",
    "Esse processo garante que, independentemente do n√∫mero de arquivos existentes na pasta, os dados ser√£o unificados de forma padronizada e consistente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5abc8ed4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hiago\\AppData\\Local\\Temp\\ipykernel_18848\\440578927.py:23: DtypeWarning: Columns (13) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(caminho_arquivo, sep=';', encoding=resultado['encoding'])\n"
     ]
    }
   ],
   "source": [
    "colunas_necess = [\n",
    "    \"Regiao - Sigla\",\n",
    "    \"Estado - Sigla\",\n",
    "    \"Municipio\",\n",
    "    \"Revenda\",\n",
    "    \"Produto\",\n",
    "    \"Data da Coleta\",\n",
    "    \"Valor de Venda\",\n",
    "    \"Unidade de Medida\",\n",
    "    \"Bandeira\",\n",
    "]\n",
    "\n",
    "df_final = pd.DataFrame()\n",
    "\n",
    "for arquivo in lista_arquivos:\n",
    "    caminho_arquivo = f\"{caminho}/DADOS/{arquivo}\"\n",
    "    \n",
    "    # Detecta o encoding do arquivo\n",
    "    with open(caminho_arquivo, 'rb') as f:\n",
    "        resultado = chardet.detect(f.read(10000))\n",
    "    \n",
    "    # Leitura do CSV\n",
    "    df = pd.read_csv(caminho_arquivo, sep=';', encoding=resultado['encoding'])\n",
    "    \n",
    "    # Remo√ß√£o de colunas desnecess√°rias\n",
    "    for coluna in df.columns:\n",
    "        if coluna not in colunas_necess:\n",
    "            df.drop(coluna, axis=1, inplace=True)\n",
    "    \n",
    "    # Concatena√ß√£o no dataframe final\n",
    "    df_final = pd.concat([df_final, df], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41b10a47",
   "metadata": {},
   "source": [
    "---\n",
    "### Convers√£o de Tipos de Dados\n",
    "Nesta etapa, realizamos a convers√£o de tipo de dados nas colunas do DataFrame com o objetivo de **otimizar o uso de mem√≥ria** e **melhorar o desempenho durante a an√°lise no Power BI**.\n",
    "\n",
    "As a√ß√µes executadas s√£o:\n",
    "\n",
    "- **Itera√ß√£o sobre todas as colunas do DataFrame**\n",
    "  Verificamos cada coluna individualmente para aplicar o tipo de dado adequado.\n",
    "\n",
    "- **Convers√£o para o tipo `category`**\n",
    "  Todas as colunas, exceto `\"Valor de Venda\"`, s√£o convertidas para o tipo `category`.  \n",
    "  Esse tipo √© ideal para colunas que possuem um n√∫mero limitado de valores distintos (como nomes de estados, cidades, bandeiras, etc.).\n",
    "\n",
    "- **Motivo da exce√ß√£o**\n",
    "  A coluna `\"Valor de Venda\"` √© mantida com tipo num√©rico (float), pois ser√° utilizada em c√°lculos e medidas no Power BI (como m√©dias e varia√ß√µes).\n",
    "\n",
    "Essa padroniza√ß√£o reduz o uso de mem√≥ria RAM e melhora o tempo de carregamento e resposta do Power BI ao manipular os dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7fa59bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "for coluna in df_final.columns:\n",
    "    if coluna != \"Valor de Venda\":\n",
    "        df_final[coluna] = df_final[coluna].astype(\"category\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f25f6b5",
   "metadata": {},
   "source": [
    "---\n",
    "### Resultado Final\n",
    "\n",
    "Ao final do processo, temos como resultado o `df_final`: um DataFrame consolidado, limpo e padronizado, contendo apenas os dados essenciais para a an√°lise dos pre√ßos de combust√≠veis no Brasil.\n",
    "\n",
    "Esse resultado cumpre exatamente o objetivo do projeto: **automatizar a prepara√ß√£o dos dados para visualiza√ß√£o no Power BI**.  \n",
    "Com essa base tratada, podemos integrar diretamente o Python ao Power BI, permitindo que o dashboard seja alimentado de forma autom√°tica e escal√°vel sempre que novos arquivos forem adicionados √† pasta de dados.\n",
    "\n",
    "Assim, garantimos uma solu√ß√£o pr√°tica e reutiliz√°vel, que reduz o retrabalho manual e facilita a an√°lise cont√≠nua da varia√ß√£o dos pre√ßos ao longo do tempo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2609450f",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "üîô [**Voltar**](https://github.com/Hiagofb/DASHBOARD_COMBUSTIVEL) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
